# XTTS-FINETUNE ðŸš‚

First, clone this repository and build the image:

```console
git clone https://github.com/veralvx/xtts-finetune xtts-finetune \
  && cd xtts-finetune \
  && podman build -f Dockerfile -t xtts-finetune
```

Start the container:

```console
podman run -it --rm --gpus=all -v ./dataset:/xtts/dataset -v ./run:/xtts/run xtts-finetune
```

If you want to use CPU only, omit `--gpus=all`.

Before fine-tuning, your directory layout should look like this:

```console
.
â”œâ”€â”€ .venv
â”œâ”€â”€ convert_audio.py
â”œâ”€â”€ dataset
â”‚   â”œâ”€â”€ metadata.csv
â”‚   â””â”€â”€ wavs
â”‚       â”œâ”€â”€ 01.wav
â”‚       â”œâ”€â”€ 02.wav
â”‚       â”œâ”€â”€ reference.wav
â”‚       â””â”€ ...
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ main.py
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ finetune.py
â”œâ”€â”€ transcribe.py
â”œâ”€â”€ uv.lock
â””â”€â”€ validate_audio.py
```

Notice:
- `.wav` files under `dataset/wavs`, with one file called `reference.wav` (~ 5s duration);
- `metadata.csv` under `dataset`

Audio files must use mono channel and 22050hz:

```
uv run main.py --validate dataset/wavs
```

```
uv run main.py --convert dataset/wavs
```

Or, using `ffmpeg`:

```
ffmpeg -i input.wav -ac 1 -ar 22050 output.wav
```

The `metadata.csv` can be obtained with:
 
```console
uv run main.py --transcribe ./dataset/wavs --lang en --model medium --device cuda
``` 

The metadata output will be under `dataset/wavs`, and it should be moved to `dataset/metadata.csv`.

Then:

```console
uv run main.py --lang en
```